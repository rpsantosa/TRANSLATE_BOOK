{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: 100%|██████████| 95/95 [05:18<00:00,  3.36s/it]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the model and tokenizer\n",
    "# model_name = 'Helsinki-NLP/opus-mt-tc-big-en-pt'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to('cuda')  # Use GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\").to(device)\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "tokenizer.src_lang = \"en_XX\"\n",
    "\n",
    "def translate_text(text):\n",
    "    # Split the text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # Create batches of sentences\n",
    "    batch_size = 16  # Increase this as needed\n",
    "    batches = [sentences[i:i+batch_size] for i in range(0, len(sentences), batch_size)]\n",
    "    progress_bar = tqdm(total=len(batches), desc=\"Translating\")\n",
    "\n",
    "    translated_text = \"\"\n",
    "    for batch in batches:\n",
    "        # Tokenize the batch\n",
    "        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=512).to('cuda')\n",
    "        \n",
    "        # Generate translations\n",
    "        translations = model.generate(**inputs)\n",
    "        \n",
    "        # Decode the translations\n",
    "        translated_batch = [tokenizer.decode(t, skip_special_tokens=True) for t in translations]\n",
    "        \n",
    "        # Add the translated batch to the overall translation\n",
    "        translated_text += \" \".join(translated_batch)\n",
    "        progress_bar.update(1)\n",
    "    progress_bar.close()\n",
    "    return translated_text\n",
    "\n",
    "\n",
    "\n",
    "def translate_and_output_sentences(text):\n",
    "    # Split the text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    batch_size = 16  # Increase this as needed\n",
    "    batches = [sentences[i:i+batch_size] for i in range(0, len(sentences), batch_size)]\n",
    "    progress_bar = tqdm(total=len(batches), desc=\"Translating\")\n",
    "\n",
    "    # Prepare the output\n",
    "    output = []\n",
    "    translated_text = \"\"\n",
    "\n",
    "    for batch in batches:\n",
    "        # Tokenize the sentence\n",
    "        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=512).to('cuda')\n",
    "        \n",
    "        # Generate translation\n",
    "        translations = model.generate(**inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"pt_XX\"])\n",
    "        \n",
    "        # Decode the translation\n",
    "        translated_batch = [tokenizer.decode(t, skip_special_tokens=True) for t in translations]\n",
    "        translated_text += \" \".join(translated_batch)\n",
    "\n",
    "        # Append original and translated sentences to the output\n",
    "        progress_bar.update(1)\n",
    "\n",
    "        for original, translated in zip(batch, translated_batch):\n",
    "            output.append(f\"Original: {original}\\nTranslated: {translated}\\n\")\n",
    "    progress_bar.close()\n",
    "    return output, translated_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the book text\n",
    "\n",
    "input_file = r\"C:\\Users\\rpsan\\iCloudDrive\\projects\\hugginface_translate_txt\\data\\Kazimierz Dąbrowski_ William TillierPositive DisintegratioMaurice Bassett.txt\"\n",
    "input_file_utf8 =  r\"C:\\Users\\rpsan\\iCloudDrive\\projects\\hugginface_translate_txt\\data\\Kazimierz Dąbrowski_ William TillierPositive DisintegratioMaurice Bassett_utf8.txt\"\n",
    "\n",
    "with open(input_file_utf8, 'r', encoding=\"utf-8\") as file:\n",
    "    book_text = file.read()\n",
    "\n",
    "# # Translate the book text\n",
    "# translated_book = translate_text(book_text)\n",
    "\n",
    "# # Save the translated text to a new file\n",
    "# with open('../output/book_translated.txt', 'w', encoding='utf-8') as file:\n",
    "#     file.write(translated_book)\n",
    "\n",
    "\n",
    "\n",
    "# Translate the book text and output sentences\n",
    "translated_output,translated_text = translate_and_output_sentences(book_text)\n",
    "\n",
    "# Save the original and translated sentences to a new file\n",
    "with open('../output/book_translated_sentences.txt_batch16_utf8_BERT', 'w', encoding='utf-8') as file:\n",
    "    for item in translated_output:\n",
    "        file.write(item)\n",
    "with open('../output/book_translated_sentences.txt_batch16_utf8_only_portuguese_BERT', 'w', encoding='utf-8') as file:\n",
    "        file.write(translated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envVS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
